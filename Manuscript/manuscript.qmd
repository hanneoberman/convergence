---
title: "Non-convergence in iterative imputation"
author: "H. I. Oberman"
abstract: "Iterative imputation has become the de facto standard to accommodate for the ubiquitous problem of missing data. While it is widely accepted that this technique can yield valid inferences, these inferences all rely on algorithmic convergence. Our study provides insight into identifying non-convergence in iterative imputation algorithms. We show that these algorithms can yield correct outcomes even when a converged state has not yet formally been reached. In the cases considered, inferential validity is achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value."
# keywords: "missing data, iterative imputation, non-convergence, MICE"
# bibliography: references.bib
execute:
  echo: false
  message: false
  warning: false
  fig_width: 6
format: pdf
bibliography: references.bib
---

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(mice)
library(ggmice)
library(ggplot2)
library(dplyr)
load("../results/out.RData")
source("../R/evaluate_estimates.R")
performance <- evaluate_est(out) 
performance[performance$term == "b0", "term"] <- "Y"
performance <- mutate(performance,
  mech = factor(mech, levels = c("MCAR", "MAR"), ordered = TRUE),
  prop = factor(prop, levels = c(0.25, 0.5, 0.75), ordered = TRUE),
  term = factor(term, levels = c("Y", "X1", "X2", "X3"), ordered = TRUE))
results <- performance |> 
  group_by(method, mech, prop, .it, term) |> 
  summarise(across(c(bias, cov, ciw, ac_mean, psrf_mean, ac_sd, psrf_sd), \(x) mean(x, na.rm = TRUE))) 
```


```{r, echo=FALSE}
# # plot results per sim rep
# performance |>
#   filter(term == "X1", mech == "MAR", method == "MICE", prop == 0.75) |>
#   ggplot(aes(x = .it, y = bias)) + #, color = as.factor(prop)
#   geom_line(aes(group = .sim)) +
#   geom_smooth(method = "loess") +
#   # labs(
#   #   title = "Bias regression coefficient",
#   #   x = "Iteration number",
#   #   y = "Bias"
#   # ) +
#   theme_classic()
```

## Aims

- can we diagnose non-convergence quantitatively?
- ~~which parameter should we track?~~
- how many iterations are needed before convergence is reached?
- how many iterations are needed before valid inferences are reached?



# Introduction

Iterative imputation has become the de facto standard to accommodate missing data in social scientific research [ref: e.g., rubin after 18 years paper? murray 2018?]. The technique enables researchers to draw valid inferences when faced with incomplete observations, without resorting to ad hoc solutions such as list-wise deletion, which may bias results. With iterative imputation, every missing cell in an incomplete dataset is imputed (i.e., filled in) algorithmically. And once the dataset is completed, the analysis of scientific interest can be performed. This analysis will yield unbiased and confidence-valid estimates if--and only if--both the missing data problem and the scientific problem are appropriately considered. The validity of this whole process naturally depends on the convergence of the algorithm that was used to generate the imputed values. Yet, there has not been a systematic study on how to evaluate the convergence of iterative imputation algorithms.

<!-- Any scientific inferences obtained after iterative imputation rely on the convergence of the imputation algorithm,  -->

Determining whether an algorithm has converged is not trivial, especially in the context of iterative imputation. Since the aim of iterative imputation is to converge to a distribution and not to a single point, the algorithm may produce some fluctuations even after it has converged. Because of this property, it may be more desirable to focus on *non*-convergence. A widely accepted practice is visual inspection of the algorithm [@ragh07; @buur18]. Diagnosing non-convergence through visual inspection may, however, be undesirable: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence [@buur18]. [add argument: "Inspection of such plots is a notoriously unreliable method of assessing convergence and in addition is unwieldy when monitoring a large number of quantities of interest, such as can arise in complicated hierarchical models" (Gelman et al., 2013, p. 285).] Therefore, a quantitative diagnostic method to assess convergence would be preferred. Fortunately, there are non-convergence identifiers for *other* iterative algorithms, but the validity of these identifiers has not been systematically evaluated on imputation algorithms. 

<!-- It is challenging to arrive upon a single point at which convergence has been reached. -->

In this study, we explore different methods for identifying non-convergence in iterative imputation algorithms. We evaluate whether these methods are able to cover the extent of the non-convergence, and we also investigate the relation between non-convergence and the validity of the inferences. We translate the results of our simulation study into guidelines for practice, which we demonstrate by means of a case study.


[TODO: integrate this section] In an empirical study with STATA, where $\widehat{R}$ was used to inform the required imputation chain length, it took as many as 50 iterations to overcome the conventional non-convergence threshold $\widehat{R}>1.2$. Yet, scientific estimates were insensitive to continued iteration from $t=5$ onward [@lace07]. We, therefore, suspect that $\widehat{R}$ may over-estimate signs of non-convergence in iterative imputation algorithms, compared to the validity of estimates. In contrast to this, signs of non-convergence can be under-estimated by $\widehat{R}$, in exceptional cases where the initial values of the algorithm are not appropriately over-dispersed [@broo98, p. 437]. In  `mice`, initial values are chosen randomly from the observed data, hence we cannot be certain of over-dispersion in the initial values. In practice, we do not expect this to cause problems for identifying non-convergence with $\widehat{R}$. 

[TODO: intrgrate this section] If the estimated quantities of scientific interest $\bar{Q}$ are unbiased and confidence-valid estimates of ${Q}$, we will conclude that the algorithm is sufficiently converged for practical purposes. In contrast to this, we defined *approximate* convergence as the most converged state that the algorithm can obtain. Continued iteration after reaching approximate convergence does not yield better mixing or stationarity (indicated by non-improving $\widehat{R}$- and $AC$-values). Continued iteration after obtaining valid inferences may lead to a more converged state of the algorithm, but not better estimates.

<!-- Iterative imputation has become the de facto standard to accommodate for missing data. The aim is usually to draw valid inferences, i.e. to get unbiased, confidence-valid estimates that incorporate the effects of the missingness. Such estimates are obtained with iterative imputation by separating the missing data problem from the scientific problem. The missing values are imputed (i.e., filled in) using some sort of algorithm. And subsequently, the scientific model of interest is performed on the completed data. To obtain valid scientific estimates, both the missing data problem and the scientific problem should be appropriately considered. The validity of this whole process naturally depends on the convergence of the algorithm that was used to to generate the imputations. -->

<!-- All inferences with imputed data rely on the convergence of the imputation algorithm. Yet, determining whether an algorithm has converged is not trivial. There has not been a systematic study on how to evaluate the convergence of iterative imputation algorithms. A widely accepted practice is visual inspection of the algorithm. Diagnosing convergence through visual inspection, however, may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence [@buur18]. Therefore, a quantitative diagnostic method to assess convergence would be preferred. -->

<!-- It is challenging to arrive upon a single point at which convergence has been reached. Since the aim is to converge to a distribution and not to a single point, the algorithm may produce some fluctuations even after it has converged. Because of this property, it may be more desirable to focus on *non*-convergence. Fortunately, there are non-convergence identifiers for other iterative algorithms, but the validity of these identifiers has not been systematically evaluated on imputation algorithms. -->

<!-- In this study, we explore different methods for identifying non-convergence in iterative imputation algorithms. We evaluate whether these methods are able to cover the extent of the non-convergence, and we also investigate the relation between non-convergence and the validity of the inferences. We translate the results of our simulation study into guidelines for practice, which we demonstrate by means of a motivating example. -->

In this paper we study different methods for assessing non-convergence in iterative imputation algorithms. We define several diagnostics and evaluate how these diagnostics could be appropriate for iterative imputation applications. We then address the impact of inducing non-convergence in iterative imputation algorithms through model-based simulation in `R` [@R]. For reasons of brevity, we only focus on the iterative imputation algorithm implemented in the popular `mice` package in `R` [@mice]. The aim of the simulation study is to determine whether unbiased, confidence-valid inferences may be obtained if the algorithm has not (yet) converged. And additionally, to evaluate the behavior and performance of several diagnostic methods to identify non-convergence. With that, we formulate an informed advice on when it is safe to conclude that the algorithm is converged *enough* for valid inferences. We translate the results of the study into guidelines for applied researchers, which may facilitate drawing valid inferences from incomplete data. 


# Background

## Some notation

Let $y$ denote an $n \times k$ matrix containing the data values on $k$ variables for all $n$ units in a sample. The data value of unit $i$ ($i = 1, 2, \dots, n$) on variable $j$ ($j = 1, 2, \dots, k$) may be either observed or missing. The number of units $i$ with at least one missing value, divided by the total number of units $n$, is called the proportion of incomplete cases $p_{\mathrm{inc}}$ in dataset $y$. The collection of observed data values in $y$ is denoted by $y_{\mathrm{obs}}$; the missing part of $y$ is referred to as $y_{\mathrm{mis}}$. For each datapoint in $y_{\mathrm{mis}}$, we sample $m \times T$ plausible values, where $m$ is the number of imputations ($\ell = 1, 2, \dots, m$) and $T$ is the number of iterations in the imputation algorithm ($t = 1, 2, \dots, T$). The state-space of the algorithm at a certain iteration $t$ may be summarized by a scalar summary $\theta$ (e.g., the average of the imputed values). The collection of $\theta$-values between $t=1$ (summarizing the state-space of the algorithm at initialization) and $t=T$ (summarizing the state-space for the imputed values) will be referred to as an 'imputation chain'. 

## Iterative imputation

Anyone who analyzes person-data may run into a missing data problem. Missing data is not only ubiquitous across science, but treating it can also be a tedious task. If a dataset contains just one incomplete observation, calculations are not defined, and statistical models cannot be fitted to the data. To circumvent this, many statistical packages employ list-wise deletion by default (i.e., ignoring incomplete observations). Unfortunately, this *ad hoc* solution may yield invalid results [@buur18]. An alternative is to apply imputation. With imputation, we 'fill in' the missing values in an incomplete dataset. Subsequently, the model of scientific interest can be fitted to the completed dataset. By repeating this process several times, a distribution of plausible results may be obtained, which reflects the uncertainty in the data due to missingness. This technique is known as 'multiple imputation' [MI; @rubin76]. MI has proven to be a powerful tool to draw valid inferences from incomplete data under many circumstances [@buur18]. 

Figure \ref{fig:diagram} provides an overview of the steps involved with MI. The missing part $y_{\mathrm{mis}}$ of an incomplete dataset is imputed $m$ times. This creates $m$ sets of imputed data $\dot y_{\mathrm{imp}, \ell}$, where $\ell = 1, 2, ..., m$. The imputed data is then combined with the observed data $y_{\mathrm{obs}}$ to create $m$ completed datasets. On each of these datasets the analysis of scientific interest is performed to estimate $Q$: the quantity of scientific interest (e.g., a regression coefficient). Since $Q$ is estimated on each completed dataset, $m$ separate $\hat{Q}_\ell$-values are obtained. Finally, the $\hat{Q}_\ell$-values are combined into a single pooled estimate $\bar{Q}$. The premise of multiple imputation is that $\bar{Q}$ is an unbiased and confidence-valid estimate of the true---but unobserved---scientific estimand $Q$ [@rubin96].
<!-- "A scientific estimand $Q$ is a quantity of scientific interest that we can calculate if we would observe the entire population" [@buur18, par 2.3.1] -->

```{r diagram, echo=FALSE, out.width="\\linewidth", include=TRUE, fig.cap="Scheme of the main steps in multiple imputation---from an incomplete dataset, to $m=3$ multiply imputed datasets, to $m=3$ estimated quantities of scientific interest $\\hat{Q}$, to a single pooled estimate $\\bar{Q}$."}

knitr::include_graphics("../figures/diagram.pdf") #adapted from [@buur18] section 1.4.1.
```

A popular method to obtain imputations is to use the 'Multiple Imputation by Chained Equations' algorithm, shorthand 'MICE'[@mice]. With MICE, imputed values are drawn from the posterior predictive distribution of the missing values. The algorithm is named after its iterative nature: a multivariate distribution is obtained by iterating over a sequence of univariate imputations. Iteration, however, also introduces a potential threat to the validity of the imputations: What if the algorithm has not converged? Are the imputations then to be trusted? And can we rely on the inference obtained on the completed data? 

These remain open questions since the convergence properties of iterative imputation algorithms have not been systematically studied [@buur18]. There is no scientific consensus on how to evaluate the convergence of imputation algorithms [@zhu15; @takahashi2017]. Moreover, the behavior of such algorithms under certain default imputation models (e.g., 'predictive mean matching') is an entirely open question [@murray2018]. Therefore, algorithmic convergence should be monitored carefully---although this is not straightforward. Iterative imputation algorithms such as MICE are special cases of Markov chain Monte Carlo (MCMC) methods. In MCMC methods, convergence is not from a scalar to a point, but from one distribution to another. The values generated by the algorithm (e.g., imputed values) will vary even after convergence [@gelm13]. Since MCMC algorithms do not reach a unique point at which convergence is established, diagnostic methods may only identify signs of *non*-convergence [@hoff09]. Several non-convergence diagnostics exist, but it is not known whether these are appropriate within the imputation framework. 

<!-- https://www-tandfonline-com.proxy.library.uu.nl/doi/pdf/10.1207/s15327906mbr3304_5?needAccess=true -->

<!-- "a weak diagnostics is better than no diagnostic at all" [@cowl96]. -->


<!-- The default number of iterations in common MI software: SPSS = 10 [(link)](https://www.ibm.com/support/knowledgecenter/SSLVMB_24.0.0/spss/mva/syn_multiple_imputation_impute.html), Mplus = 100?? [(link)](https://pdfs.semanticscholar.org/e20e/29e008592cbfbaa567931f74cdfdb5451405.pdf?_ga=2.55354671.54033656.1584698748-527613517.1584698748), Stata = 10 [(link)](https://www.stata.com/manuals13/mi.pdf, p. 139), Amelia = NA, because re-sampling, not convergence [(link)](https://cran.r-project.org/web/packages/Amelia/Amelia.pdf), en MI = 30 [(link)](https://cran.r-project.org/web/packages/mi/mi.pdf). -->





## Algorithmic non-convergence

There are two requirements for convergence of iterative algorithms: mixing and stationarity [@gelm13]. In iterative imputation algorithms, mixing implies that imputation chains intermingle nicely, and stationarity is characterized by the absence of trending across iterations. If one of the two requirements is not met, we speak of non-convergence. Without mixing, chains may be 'stuck' at a local optimum, instead of sampling imputed values from the entire predictive posterior distribution of the missing values. The distribution of imputed values then differs across imputations. This may cause under-estimation of the variance between chains, which results in spurious, invalid inferences. Without stationarity, there is trending within imputation chains. Trending implies that further iterations would yield a systematically lower or higher set of imputations. Iterative imputation algorithms that have not (yet) reached stationarity, may thus yield biased estimates.

To illustrate what non-mixing and non-stationarity look like in iterative imputation algorithms, we reproduce an example from van Buuren [-@buur18, $\S$ 6.5.2]. Figure \ref{fig:non-conv} displays two scenarios from the example. The panel on the left-hand side of the figure shows typical convergence of an iterative imputation algorithm. The right-hand side displays pathological non-convergence, induced by purposefully mis-specifying the imputation model. Each line portrays one imputation chain (i.e., the values of a scalar summary $\theta$ across iterations). The $\theta$ depicted here is the 'chain mean' of variable $j$, which is defined as the average of variable $j$ in the $m$ sets of imputations $\dot y_{\mathrm{imp}, \ell}$. 

In the typical convergence scenario, the imputation chains intermingle nicely and there is little to no trending. In the non-convergence scenario, there is a lot of trending and some chains do not intermingle. Importantly, the chain means at the last iteration (the imputed values per imputation $\ell$) are very different between the two scenarios. The algorithm with the mis-specified model yields imputed values that are on average a factor two larger than those of the typically converged algorithm. It is obvious that non-convergence in this example impacts the distribution of the imputed values per imputation $\dot y_{\mathrm{imp}, \ell}$. This effect may translate into the distribution of the $m$ sets of completed data {$y_{\mathrm{obs}}, \dot y_{\mathrm{imp}, \ell}$}, which consequently affects the estimated quantities of scientific interest $\hat{Q}_{\ell}$, and finally the pooled estimate $\bar{Q}$. $\bar{Q}$ is then a  biased, invalid estimate of $Q$. Therefore, it is important to reach algorithmic convergence in iterative imputation algorithms.


```{r non-conv, out.width='.49\\linewidth', fig.width=3.5, fig.height=3,fig.show='hold',fig.cap="Typical convergence versus pathological non-convergence (reproduced from van Buuren; 2018). Depicted are chain means of variable $j$ in each imputation ($m=5$; $T=10$). Note that any difference between the two scenarios (see e.g., the magnitude of the chain means on the y-axes) stems from the imputation algorithms---the two scenarios have equal incomplete data $y$."}

# par(mar = c(2, 2, 0.1, 0.1))
# con_plot
# non_plot

```


## Identifying non-convergence

Currently, the recommended practice for evaluating the convergence of the MICE algorithm is through visual inspection. After running the imputation algorithm for a certain number of iterations, researchers are encouraged to produce traceplots. In a traceplot, a scalar summary of the state-space of the algorithm $\theta$ is plotted against the iteration number, as depicted in Figure \ref{fig:non-conv}. The default scalar summaries to inspect for the MICE algorithm are chain means and chain variances. Non-convergence is diagnosed if the imputation chains are not freely intermingled with one another, or if the chains show definite trends [@buur18, $\S$ 6.5.2]. 
<!-- Recommendation @buur18: "the different streams should be freely intermingled with one another, without showing any definite trends". -->

<!-- Identifying non-convergence by inspecting traceplots may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence [@buur18, $\S$ 6.5.2]. Moreover, traceplots are typically only used to inspect  univariate $\theta$s. Monitoring univariate summaries of the state-space of the algorithm may be insufficient because MICE is not only concerned with a single column, but the entire multivariate distribution of the imputations. Ideally, we would monitor a multivariate $\theta$.  -->

<!-- A suggestion by @buur18 for a multivariate $\theta$ to monitor is the estimated quantity of scientific interest $\hat{Q}$, which usually is multivariate in nature. This $\theta$ is computed as the estimate in each imputation, $\hat{Q}_\ell$, across iterations. Implementing this, however, might be somewhat too technical for empirical researchers. Besides, this scalar summary is not model-independent. That is, $\theta=\hat{Q}$ is not universal to all complete data problems, while one of the advantages of iterative imputation is that the missing data problem and scientific problem are solved independently. Focusing on the convergence of outcome parameters may influence the iterative imputation procedure in the sense that the model of evaluation favors the model of interest. On these grounds, such a $\theta$ can be considered insufficient too.  -->
<!-- Researchers may "monitor some statistic of scientific interest" [@buur18, $\S$ 6.5.2].  -->

<!-- As alternative, van Buuren [-@buur18, $\S$ 4.5.2] proposed multivariate evaluation of the MICE algorithm through eigenvalue decomposition, building on the work of @mack03. This technique may yield a model-independent, multivariate scalar summary to monitor, but it is not implemented in the  `mice` package [@mice].  -->

<!-- ## A novel scalar summary -->

<!-- To monitor non-convergence in iterative imputation algorithms, we may  summarize the state-space of the algorithm with several $\theta$s. The downside to the current $\theta$s is that they either focus on the univariate state-space, or primarily track the change over the iterations of a multivariate outcome conform the scientific model of interest. Ideally, one would like to evaluate a model-independent $\theta$ that summarizes the multivariate nature of the data.  -->

<!-- We propose $\lambda_{1}$ as such a scalar summary. We define $\lambda_{1}$ as the first eigenvalue of the variance-covariance matrix in the $m$ completed datasets. The eigenvalues of a variance-covariance matrix $S$ summarize the total set of covariances in the data. Let $\lambda_{1, \ell} \geq \lambda_{2, \ell} \geq ... \geq \lambda_{j, \ell}$ be the eigenvalues of $S_\ell$ in each imputation. The first eigenvalue, $\lambda_{1,\ell}$, then summarizes the largest possible amount of covariance in each completed dataset {$y_{\mathrm{obs}}, \dot y_{\mathrm{imp}, \ell}$}. By definition, $\lambda_{1}$-values are equal to the variance of the first component that would be obtained by performing principal component analysis (PCA) on the completed data. As scalar summary, $\lambda_{1}$ has the appealing property that it is not dependent on the model of scientific interest, yet still summarizes the multivariate state-space of the algorithm. -->

<!-- In this study, we consider each of the four $\theta$s that we discussed. Namely, two univariate scalar summaries (chain mean and chain variance), a model-dependent multivariate summary $\hat{Q}$, and the novel model-independent multivariate summary $\lambda_1$. -->


## Non-convergence diagnostics

There are many diagnostic tools to identify non-convergence in iterative (MCMC) algorithms [@broo98; @elad06]. We consider only two of them that may be appropriate for imputation algorithms---one to monitor signs of non-mixing, and one for non-stationarity. As recommended by e.g. @cowl96 we will use the potential scale reduction factor $\widehat{R}$ to evaluate mixing ['Gelman-Rubin statistic'; @gelm92], and autocorrelation to diagnose trending [$AC$; @scha97; @gelm13]. With a recently proposed adaptation, $\widehat{R}$ may also serve to diagnose non-stationarity [@vehtari2021]. We will evaluate the appropriateness of both $\widehat{R}$ and $AC$. Other methods are outside the scope of this study because they e.g. assume that values within chains represent independent samples, whereas the MICE algorithm only uses the final iteration to produce imputations. 

<!-- Non-stationarity within chains may be diagnosed with e.g., autocorrelation [$AC$; @scha97; @gelm13], numeric standard error ['MC error'; @gewe92], or Raftery and Lewis's [-@raft91] procedure to determine the effect of trending on the precision of estimates.  -->

<!-- "We can use effective sample size ^neff to give us a sense of the precision obtained from our simulations. As we have discussed in Section 10.5, for many purposes it should suffice to have 100 or even 10 independent simulation draws. (If neff = 10, the simulation standard error is increased by p 1 + 1/10 = 1.05). As a default rule, we suggest running the simulation until ^neff is at least 5m, that is, until there are the equivalent of at least 10 independent draws per sequence (recall that m is twice the number of sequences, as we have split each sequence into two parts so that Rb can assess stationarity as well as mixing). Having an effective sample size of 10 per sequence should typically correspond to stability of all the simulated sequences."  @gelm13, p. 287 -->

<!-- We don't use effective sample size and MC error because they assume independent samples *within* chains. In MI, we only use the final estimate. Also, computing n_eff assumes infinitely many iterations, which is fine in Bayesian analyses where T is often at least one thousand. But the default in iterative imp is often substantially lower. -->

## Potential scale reduction factor

In 2019, Vehtari et al. proposed an updated version of the potential scale reduction factor $\widehat{R}$, originally coined in 1992. The adapted version would be better suited to detect non-mixing in the tails of distributions, and even identify non-stationarity. This version uses three transformations on the scalar summary $\theta$, before computing $\widehat{R}$-values. Namely, rank-normalization, folding, and localization. We follow Vehtari et al.'s formulation (2019, p. 5) to define $\widehat{R}$. Let $m$ be the total number of chains, $T$ the number of iterations per chain (where $T\geq2$), and $\theta$ the scalar summary of interest. For each chain ($\ell = 1, 2, \dots, m$), we estimate the variance of $\theta$, and average these to obtain within-chain variance $W$. 

\begin{align*}
W&=\frac{1}{m} \sum_{\ell=1}^{m} s_{\ell}^{2}, \text { where } s_{\ell}^{2}=\frac{1}{T-1} \sum_{t=1}^{T}\left(\theta^{(t \ell)}-\bar{\theta}^{(\cdot \ell)}\right)^{2}. 
\end{align*}

\noindent We then estimate between-chain variance $B$ (note that we diverge from the typical notation in MI, where $B$ denotes the variance between the estimated quantities of scientific interest $\hat{Q}_{\ell}$). $B$ is defined as the variance of the collection of average $\theta$s per chain: 

\begin{align*}
B&=\frac{T}{m-1} \sum_{\ell=1}^{m}\left(\bar{\theta}^{(\cdot \ell)}-\bar{\theta}^{(\cdot \cdot)}\right)^{2}, \text { where } \bar{\theta}^{(\cdot \ell)}=\frac{1}{T} \sum_{t=1}^{T} \theta^{(t \ell)} \text{, } \bar{\theta}^{(\cdot \cdot)}=\frac{1}{m} \sum_{\ell=1}^{m} \bar{\theta}^{(\cdot \ell)}. 
\end{align*}

\noindent From the between- and within-chain variances we compute a weighted average, $\widehat{\operatorname{var}}^{+}$, which over-estimates the total variance of $\theta$. $\widehat{R}$ is then obtained as a ratio between this total variance and the within-chain variance:

\begin{equation*}
\widehat{R}=\sqrt{\frac{\widehat{\operatorname{var}}^{+}(\theta | y)}{W}},
\text{ where } \widehat{\operatorname{var}}^{+}(\theta | y)=\frac{T-1}{T} W+\frac{1}{T} B.
\end{equation*}

We can interpret $\widehat{R}$ as potential scale reduction factor since it indicates by how much the variance of $\theta$ could be shrunken down if an infinite number of iterations per chain would be run [@gelm92]. The assumption underlying this interpretation is that chains are 'over-dispersed' at $t=1$, and reach convergence as $T \to \infty$. Over-dispersion implies that the initial values of the chains are 'far away' from the target distribution and each other. When the sampled values in each chain are independent of the chain's initial value, the mixing component of convergence is satisfied. The variance between chains, $B$, is then equivalent to the variance within chains, $W$, and $\widehat{R}$-values will be close to one. High $\widehat{R}$-values thus indicate non-convergence. 


## Autocorrelation

Autocorrelation is defined as the correlation between two subsequent $\theta$-values within the same chain [@lync07, p. 147]. In this study, we only consider $AC$ at lag 1, i.e., the correlation between the $t^{th}$ and $(t+1)^{th}$ iteration of the same chain. Following the same notation as for $\widehat{R}$,

\begin{equation*}
AC = \left( \frac{T}{T-1} \right) \frac{\sum_{t=1}^{T-1}(\theta_t - \bar{\theta}^{(\cdot m)})(\theta_{t+1} - \bar{\theta}^{(\cdot m)})}{\sum_{t=1}^{T}(\theta_t - \bar{\theta}^{(\cdot m)})^2}.
\end{equation*}

We can interpret $AC$-values as a measure of non-stationarity. If there is dependence between subsequent $\theta$-values in imputation chains, $AC$-values are non-zero. Positive $AC$-values occur when $\theta$-values are recurring (i.e., high $\theta$-values are followed by high $\theta$-values, and low $\theta$-values are followed by low $\theta$-values). Recurrence within imputation chains may lead to trending. Negative $AC$-values occur when $\theta$-values of subsequent iterations are less similar, or diverge from one another. Divergence within imputation poses no threat to the convergence of the algorithm---it may even speed up convergence. Complete stationarity is reached when $AC=0$. As non-convergence diagnostic, our interest is in positive $AC$-values. 


## Thresholds

It is unlikely that iterative algorithms such as MICE will achieve the ideal values of $\widehat{R}=1$ and $AC=0$. [TODO: check this! isn't it because of under-dispersion and reliance on observed data?] Because of its convergence to a distribution, the algorithm will show some signs of non-mixing and non-stationarity even in the most converged state. The aim, therefore, is to reach approximate convergence [@gelm13]. Upon approximate convergence, the imputation chains intermingle such that the only difference between the chains is caused by the randomness induced by the algorithm ($\widehat{R} \gtrapprox 1$), and there is little dependency between subsequent iterations of imputation chains ($AC \gtrapprox 0$). In practice, we diagnose non-convergence when approximate convergence is violated, i.e., when $\widehat{R}$ and $AC$ exceed a certain threshold. The conventional thresholds to diagnose non-mixing are $\widehat{R} > 1.2$ [@gelm92] or $\widehat{R} > 1.1$ [@gelm13]. @vehtari2021 proposed a much more stringent threshold of $\widehat{R} > 1.01$. The magnitude of $AC$-values may be evaluated statistically, using a Wald test with $AC=0$ as null hypothesis [@box15]. $AC$-values that are significantly higher than zero indicate non-stationarity.
<!-- Threshold Rhat > 1.1 @gelm13, p. 288 -->




# Simulation set-up

We investigate non-convergence in iterative imputation through model-based simulation in R (version 4.4.2; @R). We provide a summary of the simulation set-up in Algorithm 1; the complete script and technical details are available from [github.com/hanneoberman/MissingThePoint](https://github.com/hanneoberman/MissingThePoint) [TODO: make this a Zenodo DOI]. The number of simulation repetitions $n_{\mathrm{sim}} = 2000$.


**Algorithm 1: simulation set-up (pseudo-code)**
```
for (each simulation run) {
  simulate complete data;
  for (each missingness condition) {
    create missingness;
    for (each imputation model iteration) {
      impute missingness;
      estimate quantities of scientific interest;
      apply performance measures to the estimates;
      compute non-convergence diagnostics
} } }
```

## Aims

With this simulation, we assess the impact of non-convergence on the validity of scientific estimates obtained using the imputation package `{mice}` [@mice]. Inferential validity is reached when estimates are both unbiased and have nominal coverage across simulation repetitions ($n_{\mathrm{sim}} = 2000$). To evaluate convergence, we terminate the imputation algorithm after a varying number of iterations ($n_{\mathrm{it}} = 1, 2, ..., 100$). We differentiate between six different missingness scenarios that are defined in the data generating mechanism (see Table XYZ). 

## Data generating mechanism

\begin{table}[]
\begin{tabular}{lcr}
\hline
\multicolumn{3}{l}{Missingness conditions} \\ \hline
missing data mechanism &  & proportion of incomplete cases  \\
                       & $\times$  &                         \\
MCAR, MAR              &   & 25\%, 50\%, 75\%                \\ \hline
\end{tabular}
\end{table}


In each simulation repetition, we first generate a complete set of $n_{\mathrm{obs}} = 200$ cases, representing person-data in a multiple linear regression problem. The predictor space consists of three multivariately normal random variables, 

$$
\begin{pmatrix}
X_1\\
X_2\\
X_3
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix},
\begin{pmatrix}
1     &       &  \\
0.5   & 1     &  \\
0.5  & 0.5   & 1
\end{pmatrix}
\end{bmatrix}.
$$ 
The outcome variable $Y$ is a linear combination of the three predictors, such that for each unit $i = 1, 2,..., n_\mathrm{obs}$,

$$
Y_i = X_{1i} + X_{2i} + X_{3i} + \epsilon_i ,
$$
where $\epsilon \sim \mathcal{N}(0, 1)$. This results in a complete dataset of size $n_{\mathrm{obs}} \times p$, with units $i = 1, 2,..., n_\mathrm{obs}$ and variables $j =  Y, X_1, X_2, X_3$). Multivariate normal data are generated using the `mvtnorm` package [@mvtnorm].

Subsequently, the complete are 'amputed' (i.e., made incomplete) according to six missingness conditions. We use a $2 \times 3$ factorial design with two missing data mechanisms and three proportions of incomplete cases, see Table XYZ. We consider all possible multivariate patterns of missingness, as visualized in Figure \ref{fig:patterns}. 


```{r patterns, echo=FALSE}
source("../R/generate_datasets.R")
pat <- create_patterns(3)
is.na(pat) <- pat == 1
ggmice::plot_pattern(pat, caption = FALSE) +
  theme(axis.text.x=element_blank(), 
      axis.ticks.x=element_blank(), 
      axis.text.y=element_blank(), 
      axis.ticks.y=element_blank(),
      axis.text = element_blank()) +
  theme_void()
```


::: {.callout-note}
# Missing Data Mechanism

Missingness mechanisms refer to the probability of being missing for any given entry in a dataset. There are three distinct types of mechanisms, as defined by Rubin (1976). Roughly translated, the probability of being missing may be equal for all entries (MCAR; Missing Completely At Random), may depend on observed information (MAR; Missing At Random), or may depend on *un*observed information, making the missingness non-ignorable (MNAR; Missing Not At Random). 
:::

The missing data mechanisms under consideration are 'missing completely at random' [MCAR; @rubin87], where the probability to be missing is the same for all $n_\mathrm{obs} \times p$ cells in $y$, and a right-tailed 'missing at random' (MAR) mechanism, where the probability to be missing is a function of the observed data, and higher values are more likely to be missing. The proportion of incomplete cases $\pi_{\mathrm{inc}}$ is set to 25%, 50%, and 75%. 
We use the 'mice' package [function `mice::ampute()`; @mice] to obtain an incomplete dataset {$y_{\mathrm{obs}}, y_{\mathrm{mis}}$} for every missingness condition. 

[TODO: add left-tailed M(N)AR mechanism, md pattern, and maybe refer to ampute paper or "Dance of..." by Schouten 2018?].


## Estimands

We impute the missing data five times ($m = 5$) using Bayesian linear regression imputation with `mice` [@mice]. On each imputed dataset, we perform multiple linear regression as our analysis of scientific interest. Our estimands are the regression coefficients $\boldsymbol{\beta}$, 

$$
\hat{Y} = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3,
$$

where $\hat{Y}$ is the predicted value of the outcome. We estimate these coefficients using the `lm()` function [@R] in each imputed dataset, and subsequently pool the estimates across the five imputations using Rubin's rules [function `mice::pool()`; @mice]. 

<!-- All imputation procedures are performed using `mice` [function `mice()`; @mice], with Bayesian linear regression imputation, and five imputation chains ($m=5$). This results in $m=5$ sets of imputations for each of the conditions, $\dot y_{\mathrm{imp}, \ell}$.  -->

<!-- Subsequently, we obtain the completed data per imputation, {$y_{\mathrm{obs}}, \dot y_{\mathrm{imp}, \ell}$}, by combining the imputed data with the observed data [function `mice::complete()`; @mice]. Estimates of scientific quantities $Q$s are obtained after performing multiple linear regression [function `stats::lm()`, @R]. We pool the resulting $m$ estimates, $\hat{Q}_\ell$, into a single $\bar{Q}$ conform Rubin's rules [function `mice::pool()`; @mice]. -->


## Performance measures

\begin{table}[]
\begin{tabular}{lcr}
\hline
\multicolumn{3}{l}{Performance measures} \\ \hline
estimand &  & metric  \\
                       & $\times$  &                         \\
$\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$   &   &   bias, coverage rate, CI width  \\ \hline
\end{tabular}
\end{table}

We evaluate the pooled estimates against their true population values using the performance measures bias, confidence interval width, and coverage rate, as recommended by van Buuren (-@buur18 $\S$ 2.5.2). We calculate bias as $\bar{Q} - Q$. CR is defined as the percentage of simulation repetitions in which the 95\% confidence interval (CI) around $\bar{Q}$ covers the true estimand $Q$. Finally, we inspect CI width (CIW): the difference between the lower and upper bound of the 95\% confidence interval around $\bar{Q}$. CIW is of interest because it is a measure of efficiency. Under nominal coverage, short CIs are preferred, since wider CIs indicate lower statistical power.

<!-- We evaluate the diagnostic methods to identify non-convergence against these performance measures. $\widehat{R}$ and $AC$ should identify non-convergence in simulation conditions, where $\bar{Q}$ is *not* an unbiased, confidence-valid estimate of $Q$. -->

## Diagnostic methods


\begin{table}[]
\begin{tabular}{lcccr}
\hline
\multicolumn{5}{l}{Non-convergence diagnostics}   \\ \hline
identifier                  &          & parameter                    &          & variable                 \\
                            & $\times$ &                              & $\times$ &                          \\
AC, $\widehat{R}$           &          & chain means, chain variances &          & $Y$, $X_1$, $X_2$, $X_3$ \\ \hline
\end{tabular}
\end{table}

- non-convergence in iterative algorithms is diagnosed using an identifier and a parameter

- the parameter can be any statistic that we track across iterations, for example the average imputed value per imputation (i.e., chain means)

- the identifier is a calculation of some sort that quantifies non-convergence

- identifiers are historically focused on either non-mixing between chains or non-stationarity within chains

- the popular non-mixing identifier rhat has recently been updated by Vehtari et al, and should now work for non-stationarity as well

- just to be sure, we also use autocorrelation to quantify trending within chains

- we apply these identifiers to the two parameters that we typically evaluate after imputation using visual inspection: chain means and chain variances

We use two non-convergence identifiers---autocorrelation and $\widehat{R}$---to diagnose non-convergence in the imputation models of the four incomplete variables. For each variable we apply the two identifiers on two parameters---chain means and chain variances---and four variables, resulting in 16 sets of identifier-parameter-variable pairs.



<!-- We consider eight sets of diagnostic methods to identify non-convergence, by applying $\widehat{R}$ and $AC$ on four different $\theta$s. Univariate $\theta$s are obtained from $\dot y_{\mathrm{imp}, \ell}$. The model-independent multivariate $\theta$, $\lambda_{1}$, is obtained from {$y_{\mathrm{obs}}, \dot y_{\mathrm{imp}, \ell}$} as the variance in the first PCA component [function `stats::princomp`; R]. The model-dependent multivariate $\theta$ is one of the quantities of scientific interest: the estimated regression coefficients in {$y_{\mathrm{obs}}, \dot y_{\mathrm{imp}, \ell}$}. We apply the two non-convergence diagnostics on the $\theta$s by implementing the adapted version of $\widehat{R}$ and by manually programming the $AC$ function.  -->


# Simulation results

The following figures display the simulation results for the sixteen diagnostic identifier-parameter-variable pairs, and sixteen performance measure-variable pairs, contrasted to the number of iterations in the imputation algorithm. Within the figures, we split the results according to the missingness conditions [TODO: missingness mechanisms as line types, and proportion of incomplete cases as colors?]. Note that these results are averages of the $n_{sim} = 2000$ simulation repetitions.

## Diagnostic Methods

Figure \@ref(fig:ac) shows the autocorrelations in the chain means (panel A) and chain variances (panel B).

```{r ac_mean}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = ac_mean, color = mech, linetype = mech)) +
  geom_hline(yintercept = 0, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```


```{r ac_sd}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = ac_sd, color = mech, linetype = mech)) +
  geom_hline(yintercept = 0, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```


Autocorrelation in the chain means decreases rapidly in the first few iterations (see \@ref(fig:ac)A). The decrease is substantive until $n_{it} \geq 6$. This means that there is some initial trending within chains, but the average imputed value quickly reaches stationarity. These results hold irrespective of the missingness condition.
Autocorrelation in the chain variances show us something similar (see \@ref(fig:ac)B). The number of iterations that is required to reach non-improving autocorrelations is somewhat more ambiguous than for chain means, but generally around $n_{it} \geq 10$. We do not observe a systematic difference between missingness conditions here either. 

```{r psrf_mean}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = psrf_mean, color = mech, linetype = mech)) +
  geom_hline(yintercept = 1, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```


```{r psrf_sd}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = psrf_sd, color = mech, linetype = mech)) +
  geom_hline(yintercept = 1, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```


Figure \@ref(fig:rh) shows the potential scale reduction factor in the chain means (panel A) and chain variances (panel B).

We observe that $\widehat{R}$-values of the chain means generally decrease as a function of the number of iterations (see \@ref(fig:rh)A). An exception to this observation is a steep increase in iterations $3 \leq n_{it} \leq 5$ [TODO: interpret?? due to initialization or is there really more mixing initially??]. After the first couple of iterations, the mixing between chain means generally improves until $n_{it} \geq 30$ to $40$. There is no apparent differentiation between the missingness conditions. 
The mixing between chain variances mimics the mixing between chain means almost perfectly (see \@ref(fig:rh)B). Irrespective of the missingness condition, the $\widehat{R}$-values taper off around $n_{it} \geq 30$. 
[The rhat plots all show some initialization before the fifth iteration: is rhat useful before that??]

```{r, cache=TRUE, fig.height=9}
performance |>
  filter(method == "MICE") |>
ggplot(aes(x = .it, y = psrf_mean)) +
  geom_point(aes(group = .sim), shape = 1, alpha = 0.05) +
  geom_smooth(method = "loess") +
  facet_grid(term ~ mech + prop) +
  labs(
    title = "Potential Scale reduction Factor(PSRF) applied to chain means",
    x = "Iteration number",
    y = "PSRF"
  ) +
  theme_classic() 
```

## Performance Measures

```{r bias}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = bias, color = mech, linetype = mech)) +
  geom_hline(yintercept = 0, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```

- more missingness = more extreme bias, but also steeper decrease over iterations

```{r cov}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = cov, color = mech, linetype = mech)) +
  geom_hline(yintercept = .95, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```

- no clear trend


```{r ciw}
results |>
  filter(method == "MICE") |> 
  ggplot(aes(x = .it, y = ciw, color = mech, linetype = mech)) +
  geom_hline(yintercept = .95, alpha = 0.1) + 
  geom_line() +
  facet_grid(term ~ prop) +
  theme_classic()
```

- more missingness = wider CIs, with steeper decrease in CIW, but not stababiling at same value between missingness conditions.

In Figure \@ref(fig:perf) we show the performance measures: bias in the regression estimate (panel A), the empirical coverage rate of the regression estimate (panel C) and the average confidence interval width of this estimate (panel D).

We see that within a few iterations the bias in the regression estimate approaches zero (see \@ref(fig:perf)A). When $n_{it} \geq 6$, even the worst-performing conditions (e.g., with a proportion of incomplete cases of 75%) produce stable, non-improving estimates [regression coefficient is underestimated because there is less info to estimate the relation??]. 

Nominal coverage is quickly reached (see \@ref(fig:perf)C). After just three iterations, the coverage rates are non-improving in every missingness condition [but MNAR with 5% incomplete cases does not reach nominal coverage --\> due to bias in the estimate in combination with very narrow CI (see CIW!)].


The average confidence interval width decreases quickly with every added iteration until a stable plateau is reached (see \@ref(fig:perf)D). Depending on the proportion of incomplete cases this takes up-to $n_{it} \geq 9$. 


<!-- Our results show the performance of the MICE algorithm under different conditions of missingness and early stopping. For reasons of brevity, we only discuss results for the worst-performing estimates in terms of bias. For univariate scientific estimands ($Q=\mu$ and $Q=\sigma$) we observe the largest bias in the outcome variable $Y$. And for the regression coefficients ($Q=\beta$) the bias is most pronounced in $\beta_1$ (the effect of $X_1$ on $Y$). Since there is just one estimate for $Q=r^2$ to evaluate, we consider $Q=\mu_Y, \sigma_Y,r^2, \beta_1$. In the figures, we present all missingness conditions, but only early stopping conditions where $1 \leq T\leq50$. That is, the results are more or less stable for conditions where $T \geq 30$. Full results are available from [github.com/hanneoberman/MissingThePoint](https://github.com/hanneoberman/MissingThePoint). -->

<!-- ## Quantities of scientific interest -->

<!-- ```{r Qs, fig.height = 6.8, fig.cap="Impact of non-convergence on statistical inferences. Depicted are the bias, coverage rate (CR) and confidence interval width (CIW) of the estimates $\\bar{Q}$. We present the subset of $Q$s with the worst performance in terms of bias: the mean and standard deviation of $Y$, and the regression coefficient of $X_1$. The solid gray line depicts the targets (unbiased estimates and nominal coverage rate)."} -->

<!-- # mean_bias + sd_bias + Rsq_bias + est_bias + est_cov + est_ciw + plot_annotation(tag_levels = "A", tag_suffix = ".") + plot_layout(ncol = 2, guides = "collect")  -->
<!-- ``` -->


<!-- Figure \ref{fig:Qs} displays the influence of missingness and early stopping on the estimated quantities of scientific interest, $Q$s. In the first row of the figure, we see the bias in the estimated descriptive statistics ($Q=\mu_Y$ and $Q=\sigma_Y$; panels A and B). The second row shows the bias in the multivariate scientific estimands ($Q=r^2$ and $Q=\beta_1$; C and D). The third row consists of the coverage rate (CR) and confidence interval width (CIW) of the estimated regression coefficient ($Q=\beta_1$; E and F).  -->



<!-- ### $Q=\mu$. -->
<!-- The estimated univariate means seem unaffected by early stopping. This implies that approximately unbiased estimates may be obtained with as little as one iteration ($T \geq 1$). The bias in $\bar{Q}$ depends solely on the proportion of incomplete cases, with $p_{\mathrm{inc}}\geq.75$ leading to more extreme biases than other conditions. Completely unbiased estimates are only obtained in conditions where $p_{\mathrm{inc}}\leq.50$. This is curious since the MCAR missingness mechanism should yield unbiased univariate estimates without employing multiple imputation. Note however, that the magnitude of the bias in these conditions is small: $\mu_Y$ is maximally under-estimated by 0.02 units, while the true value of $\mu_Y$ is 25.81 ($\sigma_Y = 11.32$). We, therefore, conclude that non-convergence does not substantially impact the validity of the inferences for $Q=\mu$.  -->

<!-- ### $Q=\sigma$. -->
<!-- The estimated standard deviations are not substantially impacted by early stopping either. While the estimates for $T=1$ are not equal to other conditions, the bias is actually *less* severe than for $T\geq2$. Therefore, no iteration at all is needed to obtain approximately unbiased estimates for $Q=\sigma$. Completely unbiased estimates are only obtained in conditions where $p_{\mathrm{inc}}=.05$. The other missingness conditions are impacted by non-convergence in the order of increasing $p_{\mathrm{inc}}$. Similar to $Q=\mu$, the magnitude of the bias in $Q=\sigma$ is negligible. We, therefore, conclude that neither one of the univariate $Q$s is affected substantially by non-convergence. -->

<!-- ### $Q=r^2$. -->
<!-- In contrast to the univariate estimates, the estimated coefficient of determination is clearly affected by early stopping, see Figure \ref{fig:Qs}C. The bias in $Q$ appears to decrease with each additional iteration, while the magnitude of the bias still depends on the proportion of incomplete cases. Therefore, we see that the number of iterations that is necessary to reach stable, non-improving estimates also differs across missingness conditions. In conditions where $p_{\mathrm{inc}}=.05$, estimates are unbiased after one iteration. The highest number of iterations necessary to reach approximate unbiasedness is seven ($p_{\mathrm{inc}}=.95$; $T\geq7$). Completely unbiased estimates are only obtained in conditions where $p_{\mathrm{inc}}\leq.25$. This implies that even for biased estimates, $T\geq7$ would suffice to reach a stable solution. -->


<!-- ### $Q=\beta$. -->
<!-- For the estimated regression coefficients, we first consider bias, before discussing CR and CIW. As Figure \ref{fig:Qs}D shows, the bias in $\bar{Q}$ is affected by both the proportion of incomplete cases and the number of iterations. Similar to $Q=r^2$, we observe approximately unbiased estimates after at least one, and at most seven iterations, depending on $p_{\mathrm{inc}}$. Completely unbiased estimates are only obtained in conditions where $T\geq3$ and $p_{\mathrm{inc}}\leq .50$. Despite persistent bias in conditions where $p_{\mathrm{inc}}\geq.75$, the coverage rates are non-improving after just three iterations, irrespective of $p_{\mathrm{inc}}$. We conclude that any condition where $T\geq3$ yields approximately nominal coverage rates, and thus confidence-valid estimates. -->
<!-- The nominal coverages can be explained by the CIWs. The CIWs depicted in Figure \ref{fig:Qs}F are conform the theoretical foundation of MI: CIs are wider in conditions with higher proportions of incomplete cases (i.e., there is less information in the data, and thus more uncertainty due to missingness). Since conditions with higher $p_{\mathrm{inc}}$ result in both wider CIs and more severe bias, the true value of $Q$ may be included in the CI, despite of the bias in $\bar{Q}$. -->
<!-- We conclude that approximately unbiased estimates may be obtained in conditions where $T\geq7$, whereas confidence-valid estimates require at most three iterations.  -->


<!-- ### Summary.  -->
<!-- These results demonstrate that the estimates of univariate scientific estimands $Q$ are not impacted by early stopping of the MICE algorithm or the proportion of incomplete cases in $y$. Unbiased estimates may be obtained after just one iteration. Multivariate estimates, by contrast, are affected by both the number of iterations and the proportion of missing cases. Completely unbiased estimates are only obtained under low to moderate missingness ($p_{\mathrm{inc}}\leq.50$), after at most three iterations. We observe approximately unbiased estimates after at most seven iterations (for any $p_{\mathrm{inc}}$ considered). This implies that the algorithm produces stable, non-improving estimates when $T\geq7$.  -->


<!-- ## Non-convergence diagnostics -->

<!-- Figure \ref{fig:rhats-acs} displays results of the eight sets of diagnostic methods. The two columns in the figure represent the  non-convergence diagnostics $\widehat{R}$ and $AC$; the rows depict the four scalar summaries $\theta$ under consideration. The first row in Figure \ref{fig:rhats-acs} thus shows $\widehat{R}$ and $AC$ applied on the chain means (panels A and B). In the second row, we see the diagnostics applied on the chain variances (C and D). The third row depicts the same for $\theta=\hat{Q}$ (E and F). And in the last row, we see the diagnostics applied on the novel $\theta$ to consider: $\lambda_1$ (G and H). We evaluate the performance of the methods by establishing whether they correctly identify conditions in which $\bar{Q}$ is *not* an unbiased, confidence-valid estimate of $Q$. As we concluded in the last section, substantially biased estimates were only obtained in conditions where $T\leq6$, and we observed non-nominal coverage only when $T\leq3$. However, there was some persistent bias in conditions where $p_{\mathrm{inc}}\geq.75$, irrespective of the number of iterations. If the diagnostic methods are appropriate for MI, we will detect non-convergence in these conditions. -->


<!-- ```{r rhats-acs, fig.height = 6.8, fig.cap="Non-convergence diagnostics $\\widehat{R}$ and $AC$ applied on several $\\theta$s. The left-hand side of the figure contains $\\widehat{R}$-values, and the right-hand side contains $AC$-values. Depicted in the rows are the scalar summaries $\\theta$: chain mean, chain variance, the quantity of scientific interest $\\hat{Q}$, and the first eigenvalue of the variance-covariance matrix $\\lambda_1$. The dashed gray lines depict the different thresholds to diagnose non-convergence."} -->

<!-- # mean_Rh + mean_AC + var_Rh + var_AC + est_Rh + est_AC + PCA_Rh + PCA_AC + plot_layout(guides = "collect", ncol = 2) + plot_annotation(tag_levels = "A", tag_suffix = ".") -->
<!-- ``` -->


<!-- ### $\theta=$ chain mean.  -->
<!-- As expected, $\widehat{R}$ lowers with the number of iterations. After an initial 'dip' in conditions where $T=3$, we observe a gradual decrease in $\widehat{R}$-values with every additional iteration. This implies improving convergence, until the decrease tapers off after 30 to 40 iterations. Using the threshold $\widehat{R}>1.2$, we diagnose non-convergence in conditions where $T\leq7$, whereas the threshold 1.1 is exceeded when $T\leq13$. The very strict threshold 1.01 is surpassed in all conditions $T\leq100$ (not shown), and therefore not informative under the current specifications. With this set of methods, we fail to identify non-convergence in conditions with persistent bias due to missingness. -->

<!-- Similarly, $AC$ decreases with higher $T$s. $AC$ indicates improving stationarity in conditions where $T\leq6$. The $AC$-values do not exceed the threshold defined by statistical significance of the $AC$-values. Based on this threshold, we fail to diagnose non-convergence in any condition.   -->


<!-- ### $\theta=$ chain variance. -->
<!-- As panel C and D in Figure \ref{fig:rhats-acs} show, the results of this set of methods are highly similar to those with chain means as $\theta$. This holds for both $\widehat{R}$ and $AC$. The only apparent difference is in the $\widehat{R}$ threshold 1.1. Instead of diagnosing non-convergence in conditions where $T\leq13$, we now diagnose this when $T\leq11$. All other observations are equivalent. -->


<!-- ### $\theta=\hat{Q}$. -->
<!-- The $\widehat{R}$-values in Figure \ref{fig:rhats-acs}E show a similar trend across iterations as we observed for chain means and chain variances. Only conditions with a very high  proportion of missing cases ($p_{\mathrm{inc}}\geq.75$) diverge from the earlier observations. The $\widehat{R}$-values in these conditions do not taper off after 30 to 40 iterations, but rather between 40 to 50 iterations. The highest number of iterations at which non-convergence is still diagnosed according to the thresholds are 11 for $\widehat{R}>1.2$, 23 for $\widehat{R}>1.1$, and 100 for $\widehat{R}>1.01$ (not shown).  -->

<!-- The $AC$-values also follow earlier observations, with the same exception as for $\widehat{R}$ (when $p_{\mathrm{inc}}\geq.75$). To reach maximum stationarity, these conditions require at most seven iterations instead of five. Moreover, the $AC$-values in these conditions exceed the threshold to diagnose non-convergence. Interestingly, this only occurs after ten or even thirty iterations, while we would expect more signs of trending early in the iterations. Since the $AC$-values are not improving or worsening when $T\geq7$, we conclude that non-convergence is incorrectly diagnosed based on the threshold. -->

<!-- ### $\theta=\lambda_{1}$. -->
<!-- Once again, we observe $\widehat{R}$-values with a similar trend across iterations. For this $\theta$, the results differ only in the most extreme missingness condition. When $p_{\mathrm{inc}}=.95$, we diagnose non-convergence at $T\leq9$ according to $\widehat{R}>1.2$, $T\leq19$ for $\widehat{R}>1.1$, and 100 for $\widehat{R}>1.01$ (not shown). The 'dip' in $\widehat{R}$-values at $T=3$ is even more pronounced than with other $\theta$s. Some $\widehat{R}$-values even overcome the threshold of 1.2. If we would terminate the algorithm at $T=3$, we would incorrectly conclude that the algorithm reached approximate convergence, according to this threshold. -->

<!-- $AC$-values are systematically higher for this $\theta$ than for other scalar summaries. The $AC$-values also taper off at a later point in the iterations. This suggests improving stationarity up-to ten, or even thirty iterations, depending on the missingness condition. According to the threshold, we should diagnose non-convergence at some point in any missingness condition. However, this occurs only after reasonably decreasing $AC$-values are obtained---again suggesting that non-convergence is incorrectly diagnosed.    -->


<!-- ### Summary.  -->
<!-- Overall, the methods to diagnose non-convergence perform as expected: they indicate more signs of non-convergence in conditions with worse performance in terms of bias and confidence-validity. Under low to moderate missingness, it does not seem to matter on which $\theta$ the non-convergence diagnostics are applied. If we look at complete unbiasedness, however, we notice that univariate $\theta$s fail to diagnose the persistent bias in conditions where $p_{\mathrm{inc}}\geq.75$.  -->

<!-- The number of iterations necessary to obtain approximately unbiased, confidence-valid estimates corresponds to the $\widehat{R}$ threshold 1.2. The thresholds 1.1 and 1.01 seem too strict compared to the validity of the inferences. The threshold to diagnose significant $AC$-values does not appear to be appropriate in at least the current set-up, and perhaps in iterative imputation in general. A better heuristic to diagnose non-stationarity with $AC$ may be through evaluation of the $AC$-values across iterations: If the values do not substantially decrease with $T$, approximate stationarity may be concluded. -->

# Discussion

Our study found that---in the cases considered---inferential validity was achieved after five to ten iterations, much earlier than indicated by the non-convergence identifiers. Of course, it never hurts to iterate longer, but such calculations hardly bring added value.


- Convergence diagnostics keep improving substantially until n_it = 20-30

- Performance measures do not improve after n_it = 9

- [methodological explanation is that rhat and ac have a lag (few it to inform your statistic) --\> will always indicate convergence slower than inferential validity is reached]

In short, the validity of iterative imputation stands or falls with algorithmic convergence---or so it's thought. We have shown that iterative imputation algorithms can yield correct outcomes even when a converged state has not yet formally been reached. Any further iterations just burn computational resources without improving the statistical inferences. 


<!-- Before drawing any further conclusions, let us first return to the hypotheses that we formulated for the simulation study.  -->

<!-- 1. As hypothesized, biased, invalid estimates of quantities of scientific interest occurred more often in simulation conditions with a high proportion of incomplete cases $p_{\mathrm{inc}}$ and a low number of iterations $T$. However, this only holds for multivariate scientific estimands. Univariate $Q$s are not substantially impacted by early stopping. -->

<!-- 2. Our results also support the hypothesis that $\widehat{R}$ and $AC$ would correctly identify signs of non-convergence. We observed higher $\widehat{R}$ and $AC$-values in conditions where $\bar{Q}$s were biased, invalid estimates of $Q$s.  -->

<!-- 3. We hypothesized that the recommended thresholds to diagnose non-convergence with $\widehat{R}$ would be too stringent for iterative imputation applications, compared to the number of iterations necessary for statistically valid inferences. This hypothesis is partially supported: On the one hand, the 1.01 and 1.1 thresholds indeed seem to over-estimate the signs of non-convergence. On the other hand, however, we observed that the traditional threshold of 1.2 roughly corresponds to the number of iterations required to obtain approximately unbiased, confidence-valid estimates.  -->

<!-- 4. The results of this simulation study do not support the hypothesis that high $AC$-values are implausible in iterative imputation algorithms with typical convergence. We expected that the randomness induced by the algorithm would effectively mitigate the risk of dependency within chains after a few iterations. In this study, however, we observed $AC$-values as high as theoretically possible ($AC=1$). The second part of the hypothesis is not refuted: the $AC$-values did decrease quickly as a function of $T$. -->

<!-- 5. Our final hypothesis was that multivariate $\theta$s would be better at detecting non-convergence than univariate $\theta$s. Multivariate $\theta$s did indeed show superior performance under certain conditions (e.g., multivariate $Q$s and extreme missingness). -->



<!-- ## Conclusion -->

<!-- We have shown that non-convergence in iterative imputation algorithms goes hand in hand with biased, invalid estimates---at least for the multivariate quantities of scientific interest considered in this study. Identifying non-convergence may be a crucial aspect of drawing valid statistical inferences from incomplete data. We concluded that the current practice of visually inspecting non-convergence through traceplots of univariate scalar summaries of the state-space of the algorithm does not suffice. We, therefore, considered eight sets of diagnostic methods to identify non-convergence: two non-convergence diagnostics ($\widehat{R}$ and $AC$), two univariate $\theta$s, a model-dependent multivariate $\theta$, and a novel, model-independent multivariate $\theta$. Signs of non-convergence due to early stopping were identified with each of the methods. Bias due to high proportions of incomplete cases was only identified with multivariate $\theta$s.  -->

<!-- Since we obtained approximately unbiased, confidence-valid estimates after at most seven iterations, we conclude that the $\widehat{R}$ threshold 1.2 is the most appropriate diagnostic cut-off. The threshold to diagnose non-stationarity with $AC$ does not seem to apply. However, some signs of non-convergence were detected in simulation conditions many more iterations. Under moderate missingness ($p_{\mathrm{inc}}\leq.50$) $\widehat{R}$-values decreased substantially until 30 to 40 iterations, which implies that mixing in the algorithm still improved with each additional iteration. $AC$-values only improved until about six iterations, suggesting minimal improvement in trending was obtained beyond $T=6$ in the current simulation set-up.  -->

<!-- The main finding of this study is that valid inferences may be obtained much quicker than approximate algorithmic convergence is reached. Under the current specifications, univariate $Q$s did not require algorithmic convergence at all. They are unbiased almost instantly after the algorithm is initiated. Approximately unbiased, confidence-valid estimates of multivariate $Q$s were obtained after a maximum of seven iterations. Continued iterations beyond $T = 7$ did not yield better estimates. -->


<!-- ## Implications -->

<!-- Iterative imputation algorithms such as MICE are known to yield valid results under severe missingness. With this study, we have shown that valid inferences can also be obtained under a combination of severe missingness and early stopping. Based on our results, the traditional threshold of $\widehat{R}> 1.2$ would be appropriate for diagnosing non-convergence in iterative imputation algorithms. This is, however, not in line with the recent recommendations by @vehtari2021 to lower the threshold to 1.01. The discrepancy between our conclusion and Vehtari et al. may be explained by the nature of iterative imputation algorithms. In the limit, iterative imputation algorithms have the same characteristics as other MCMC algorithms. But, in imputation procedures, a part of the distribution is already determined by the observed data, whereas the entire distribution is unknown in many other MCMC applications. Since we combine a known distribution with an unknown distribution, valid estimates may be reached much sooner. Convergence may, therefore, be diagnosed at a less stringent threshold.  -->


<!-- The threshold to diagnose non-convergence with $AC$ does not seem appropriate at all in iterative imputation algorithms. A better diagnostic cut-off would be the number of iterations at which an additional iteration does not substantially decrease the $AC$-values. In practice, this implies that the default number of iterations in MICE is not sufficient. $AC$ can only be computed if $T\geq3$, while the current default in the `mice` package is five [@mice]. Identifying a reasonable decrease in the $AC$-values across iterations (an 'elbow') requires more than three observations. We, therefore, suggest to increase the default number of iterations in MICE to ten. The computational cost of five additional iterations has become less of a burden since MICE was introduced [-@mice]. Moreover, an increased number of iterations would grant the opportunity to exclude the first iterations from evaluation with $\widehat{R}$. Excluding $T\leq3$ provides empirical researchers with a less ambiguous heuristic to diagnose non-convergence, because we expect the initial 'dip' in $\widehat{R}$-values to disappear. -->

<!-- The potential scale reduction factor metric assumes over-dispersed starting values of the iterative algorithm. The MICE algorithm does not start in an over-dispersed state.  MICE does not rely on starting values for parameters at all: instead, the algorithm is initialized based on a 'zeroth' iteration, where missing values are filled in using a random draw from observed values. Under MCAR, this would typically not yield an over-dispersed state at all, depending on the parameter of interest: means and variances are not affected, regression coefficients and other multivariate parameters start biased downwards (because sampling starting values distorts multivariate structures in the data). Whether this accrues to over-dispersion is questionable. Under MAR (or MNAR), some over-dispersion might occur when the observed data and imputed data differ impactfully. The algorithm should 'escape' the initial state (based on the starting values drawn from observed data alone). Over iterations, all parameters of interest (e.g. means, variances, multivariate estimands) should converge towards a stable state. Moreover, the MICE algorithm is initialized with more information than a typical MCMC algorithm: parameter estimates depend not only on imputed data, but also on observed data that does not change over iterations. Therefore, the initial state of the algorithm is 'too good' to observe a relative decrease in $\widehat{R}$. -->

<!-- ## Recommendations for empirical researchers -->

<!-- Based on our results, we formulate several recommendations for empirical researchers who employ iterative imputation to draw inferences from incomplete data. We suggest that non-convergence may still be evaluated visually, but in addition to inspecting univariate summaries of the state-space of the algorithm ($\theta$s, e.g., chain means), multivariate $\theta$s should be considered. We propose the following steps: -->

<!-- 1. First, check traceplots of the default $\theta$s (e.g., chain means and chain variances) for signs of pathological non-convergence. Adjust the imputation model if necessary.  -->

<!-- 2. Subsequently, decide which multivariate $\theta$s to track across iterations. The novel $\theta$ that we propose in this study, $\lambda_1$ is scientific model-independent and may thus always be employed. Alternatively, specify your own scalar summary of interest [see e.g., @buur18]. Monitor these $\theta$s through visual inspection, or using a non-convergence diagnostic. -->

<!-- 3. Compute non-convergence diagnostics $\widehat{R}$ and $AC$. Do *not* use the original implementation to calculate $\widehat{R}$ [@gelm92], or the `R` function `stats::acf()` to compute $AC$ [@R]. Instead, calculate $\widehat{R}$ conform @vehtari2021 and compute autocorrelations manually (see e.g., [github.com/hanneoberman/MissingThePoint](https://github.com/hanneoberman/MissingThePoint)).  -->

<!-- 4. And finally, use the threshold $\widehat{R}>1.2$ to diagnose non-mixing, and assess stationarity by plotting the autocorrelation over iterations. Keep iterating until the threshold for mixing is overcome, and until reasonably decreasing asymptotic $AC$-values are obtained. At that point, inferences are unlikely to improve by continued iteration.  -->



<!-- ## Limitations -->

<!-- Much remains unknown about non-convergence in iterative imputation algorithms. Even though we have demonstrated the appropriateness of $\widehat{R}$ and $AC$ as non-convergence diagnostics in this study, results may not extrapolate to other situations. In this study, we only considered the iterative imputation algorithm implemented in the `mice` package. The performance of the non-convergence diagnostics may depend on the characteristics of this specific algorithm. A potential threat to the ability to detect non-mixing with $\widehat{R}$, for example, is the assumption of over-dispersed initial states. In `mice`, the algorithm is initiated by sampling starting values at random from the observed data, whereas other imputation software may use the average of the observed cases. The latter could lead to a systematical under-estimation of the variability in the first few iterations and violate the assumption of over-dispersion. In those cases, it would be more difficult to detect non-convergence with $\widehat{R}$. Therefore, the diagnostic might not be  appropriate outside of `mice`.  -->

<!-- Moreover, the current simulation conditions were restricted to a single missingness mechanism. Proper performance under a 'missing completely at random' (MCAR) mechanism is a necessary condition for any missing data method. It does not, however, guarantee equal performance under different missingness mechanisms. Future research should determine the performance of $\widehat{R}$ and $AC$ under 'missing at random' and 'missing not at random' mechanisms. Another parameter to consider in future research is the choice of the imputation method. As @murray2018 concluded, the behavior of algorithms such as MICE under certain default imputation models is still an open research question. We have investigated the behavior of MICE under only one type of model (Bayesian linear regression). Different imputations models might converge more poorly.  -->

<!-- The imputation model may even be mis-specified on purpose, to induce non-convergence of different severity levels. Qualitatively, we have shown that $\widehat{R}$ and $AC$ are appropriate under clear violation of convergence. Quantitatively, we have only demonstrated that they can identify signs of non-convergence under MCAR, when the imputation model is a correctly specified Bayesian linear regression model.  \newline \newline -->


<!-- \noindent In short, we have shown that iterative imputation algorithms can yield correct outcomes, even when a converged state has not yet formally been reached. Any further iterations would then burn computational resources without improving the statistical inferences. Our study found that---in the cases considered---inferential validity was achieved after five to ten iterations, much earlier than indicated by the $\widehat{R}$ and AC diagnostics. Of course, it never hurts to iterate longer, but such calculations hardly bring added value. -->


# Case Study

We use empirical incomplete data: the `boys` dataset from the `mice` package, which contains health-related data for 748 Dutch boys [@mice]. Say, we're interested in the relation between children's heights and their respective ages, we could use a linear regression model to predict `hgt` from `age`. However, as figure XYZ shows, the variable `hgt` is not completely observed. To be able to analyze these data, we need to solve the missing data problem first.

```{r boys, echo=FALSE}
ggmice::ggmice(mice::boys, ggplot2::aes(age, hgt, shape = .where)) + 
  ggplot2::geom_point() + 
  ggplot2::scale_shape_manual(values = c(1, 4)) +
  ggplot2::labs(x = "Age (years)", y = "Height (cm)", shape = "")
```

The incomplete height variable can be imputed based on auxiliary variables, such as weight. After imputation with `mice`, one would conventionally inspect the trace plots for signs of non-convergence.

```{r trace, echo=FALSE}
imp <- mice::mice(mice::boys[, c("age", "hgt", "wgt")], maxit = 20, printFlag = FALSE)
plot_trace(imp, "hgt") 
```

A mis-specified imputation model can lead to non-convergence in the imputation algorithm. 

```{r, echo=FALSE}
# impute missingness with mis-specified imputation model
meth <- make.method(boys) #define imputation model
meth["bmi"] <- "~I(wgt / (hgt / 100)^2)" #mis-specify model
nonconv <- mice( #impute missingness
  boys,
  meth = meth,
  maxit = 20,
  print = FALSE,
  seed = 60109
)

# # save results
# save(nonconv, file = "Results/example_nonconv.Rdata")


#####################
# Regular convergence
#####################

# impute missingness with correctly specified imputation model
pred <- make.predictorMatrix(boys) #mend imputation method
pred[c("hgt", "wgt"), "bmi"] <- 0 #remove dependency
conv <- mice( #impute missingness
  boys,
  meth = meth,
  pred = pred,
  maxit = 20,
  print = FALSE,
  seed = 60109
)

plot_trace(nonconv, "hgt") 
# plot_trace(conv, "hgt") +
#   ggplot2::labs(title = "Typical convergence")
```



- We use real data: the `boys` dataset from the `mice` package

- We are interested in predicting age from the other variables, in particular in the regression coefficient of `hgt`

- We compare non-convergence identified using visual inspection versus rhat in the chain variances, scientific estimate and lambda.

- The figures show results of a `mice` run with 20 iterations but otherwise default settings.

From the traceplot of the chain means (see \@ref(fig:case)A) it seems that mixing improves up-to 10 iterations, while trending is only apparent in the first three iterations. 



This figure (\@ref(fig:case)B) shows that 7 iterations are required before the $\widehat{R}$-values of the chain means drop below the threshold for non-convergence.


The $\widehat{R}$-values for the scientific estimate reaches the threshold much sooner, when $n_{it}=14$ (see \@ref(fig:case)C).


According to the $\widehat{R}$-values with $\lambda$ as parameter, at least 15 iterations are required (see \@ref(fig:case)D).

Before we evaluate the performance of the non-convergence diagnostics $\widehat{R}$ and $AC$ quantitatively through simulation, we first assess their appropriateness qualitatively. We do this by applying them to the example of pathological non-convergence that we reproduced form @buur18. Ideally, the diagnostics are as informative as, or better than visual inspection of the traceplots. The methods should at least indicate worse performance (higher $\widehat{R}$- and $AC$-values) for the scenario with pathological non-convergence, compared to the typically converged algorithm. 

Each panel in Figure \ref{fig:diagnostics} depicts one method to identify non-convergence, applied to the two scenarios from Figure \ref{fig:non-conv}. Panel A consists of the two traceplots that may be evaluated through visual inspection. Panel B shows two versions of the $AC$: the default calculation with `R` function `stats::acf()` [@R], and manual calculation as the correlation between $\theta$ in iteration $t$ and $\theta$ in iteration $t+1$. Panel C displays the traditional computation of $\widehat{R}$ conform Gelman and Rubin (original), and in panel D we see $\widehat{R}$ as computed by implementing Vehtari et al.'s recommendations (adapted). 


```{r diagnostics, fig.cap="Convergence diagnostics applied on the two scenarios from Figure 2 ($\\theta$ = chain mean of variable $j$; $m=5$; $T=10$). Depicted are the imputation chains in panel A, two versions of the autocorrelation in panel B, and two versions of $\\widehat{R}$ in panels C and D."}

# (theta + plot_layout(guides = "collect", ncol = 2)) + (ac_both + plot_layout(guides = "keep")) + old_rhat + new_rhat +  plot_annotation(tag_levels = "A", tag_suffix = ".") 
```


<!-- By visually inspecting the imputation chains in panel A, we conclude that the two scenarios show very different convergence. The typically converged algorithm initially portrays some signs of non-mixing (around $t=2$), but intermingles nicely overall. Additionally, there is very little trending in this scenario. The algorithm with pathological non-convergence shows severe non-mixing, although this gradually improves (beyond $t=7$). In this scenario, we see a lot of trending initially (up-to $t=6$), after which the chains reached a somewhat more stationary state.  -->

<!-- When we look at panel B, we conclude something weird. The $AC$-values calculated with the default function indicate equal performance for the typical convergence and the pathological non-convergence scenarios (up-to $t=5$), while there is obvious trending in the $\theta$s of the latter. Moreover, the best convergence (as indicated by the lowest $AC$-value) is observed at $t=2$ for both scenarios. However, if we look at the chain means of the non-convergence scenario, there should be signs of trending  up-to iteration number seven. After consulting the documentation on `stats::acf()`, we conclude that this $AC$ function is not suited for iterative imputation algorithms. The function is optimized for performance when $t\geq50$ [@box15], while the default number of iterations in iterative imputation is often much lower. Therefore, we compute $AC$ manually (presented with solid lines in panel B). These $AC$-values behave as expected from visual inspection. We will, therefore, only consider manually calculated $AC$-values as non-convergence diagnostic. -->

<!-- In panel C, we do not see a lot of difference between the two scenarios either. The $\widehat{R}$-values are even somewhat lower for the non-convergence scenario than for the typical convergence. And apparently, the large variance causes an increase in $\widehat{R}$-values. Convergence seems to worsen with every additional iteration. This paints the wrong picture. An explanation for this behavior may be that under severe trending, the assumption of over-dispersion is violated. Taken together, this version of $\widehat{R}$ is sub-optimal for assessing non-convergence in MI. -->

<!-- The $\widehat{R}$-values in panel D are conform our expectations. The adapted version of $\widehat{R}$ depicted here does indicate less signs of non-convergence as the number of iterations goes up. An exception to this general trend is the 'dip' around $t=3$. As diagnostic tool, this may lead to the incorrect conclusion that the non-convergence scenario has superior convergence compared to the typical scenario. We can explain this as a downside of the low number of iterations in iterative imputation: the adapted version of $\widehat{R}$ can only be completely employed if the number of iterations is at least four. Otherwise, it is not possible to perform all three transformations to the chains of $\theta$-values. For low values of $t$, the $\widehat{R}$-values conform Vehtari et al. are, therefore, more similar to the original $\widehat{R}$ by Gelman and Rubin. -->

<!-- In short, both non-convergence diagnostics have a version that may be appropriate for iterative imputation algorithms. We will compute $\widehat{R}$ conform @vehtari2021, and calculate $AC$ manually. The diagnostics will be applied to the four types of scalar summaries $\theta$ described earlier: chain means, chain variances, a quantity of scientific interest, and the first eigenvalue of the variance-covariance matrix. We evaluate the performance of these eight sets of diagnostic methods (two diagnostics; four $\theta$s) through simulation. -->

