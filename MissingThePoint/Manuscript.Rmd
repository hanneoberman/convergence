---
title: "Missing The Point: Non-Convergence in Iterative Imputation Algorithms"
author: "Hanne Oberman"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  bookdown::pdf_document2
bibliography: ref.bib
abstract: "Iterative imputation has become the de facto standard to accommodate for the ubiquitous problem of missing data. While it is widely accepted that this technique can yield valid inferences, these inferences all rely on algorithmic convergence. Our study provides insight into identifying non-convergence in iterative imputation algorithms. We show that these algorithms can yield correct outcomes even when a converged state has not yet formally been reached. In the cases considered, inferential validity is achieved after five to ten iterations, much earlier than indicated by diagnostic methods. We conclude that it never hurts to iterate longer, but such calculations hardly bring added value."

---

```{r setup, include=FALSE}
# environment
knitr::opts_chunk$set(echo = TRUE, fig.width = 10, fig.width = 10)
library(tidyverse)
library(mice)
library(patchwork)
# load data
load("Data/parameters.Rdata")
load("Data/results_25_75.Rdata")
load("Data/example.Rdata")
load("Data/example_chains.Rdata")

# sim results plotting function
simulation_plot <- function(d = results, v, name, ...){
  d %>%
    mutate(
      p = factor(p, levels = c(0.25, 0.5, 0.75), labels = c("25%", "50%", "75%")),
      mech = factor(mech, levels = c("MCAR", "MAR", "MNAR")),
      est = est - 1,
      rsq = rsq - 0.5
      ) %>% 
  ggplot(aes(
    x=.data[["it"]], 
    y=.data[[v]], 
    color = .data[["p"]], 
    shape = .data[["mech"]], 
    linetype = .data[["mech"]]
    )) +
  geom_line(na.rm = TRUE) +
  labs(
    x = "Number of iterations",
    y = name,
    color = "Incomplete \n cases", 
    shape = "Missingness \n mechanism", 
    linetype = "Missingness \n mechanism") +
  theme_classic() + 
  scale_color_manual(values = c(
    #"#009E73", #green
    "#56B4E9", #blue
    #"#E69F00", #orange
    "#F0E442", #yellow
    "#D55E00"  #red
  )) 
  # scale_color_manual(values = c(
  #   # '#228833', #green
  #   # '#66CCEE', #blue
  #   '#CCBB44', #yellow
  #   '#EE6677',  #red~ish
  #   '#AA3377'  #maroon
  #   ))
}

# plot each var of example data
casestudy_plot <- function(d = example, v, name, ...){
ggplot(d, aes(x = .data[["it"]], y = .data[[v]])) +
  geom_line(na.rm = TRUE) +
    labs(x = "Iteration number",
         y = name) +
  theme_classic()
}

```



# Notation in this manuscript: 

- square brackets are comments for myself/stuff I need to review

- bullet points are things I need to expand

- asterisks denote a weak 'segway' between sentences


# Intro

Iterative imputation has become the de facto standard to accommodate for missing data. The aim is usually to draw valid inferences, i.e. to get unbiased, confidence-valid estimates that incorporate the effects of the missingness. Such estimates are obtained with iterative imputation by separating the missing data problem from the scientific problem. The missing values are imputed (i.e., filled in) using some sort of algorithm. And subsequently, the scientific model of interest is performed on the completed data. To obtain valid scientific estimates, both the missing data problem and the scientific problem should be appropriately considered. The validity of this whole process naturally depends on the convergence of the algorithm that was used to to generate the imputations. 

All inferences rely on the convergence of the imputation algorithm, yet determining whether an algorithm has converged is not trivial. There has not been a systematic study on how to evaluate the convergence of iterative imputation algorithms. A widely accepted practice is visual inspection of the algorithm, although diagnosing convergence through visual inspection may be undesirable for several reasons: 1) it may be challenging to the untrained eye, 2) only severely pathological cases of non-convergence may be diagnosed, and 3) there is not an objective measure that quantifies convergence [@buur18]. Therefore, a quantitative diagnostic method to assess convergence would be preferred.*

It is challenging to arrive upon a single point at which convergence has been reached.* Since the aim is to converge to a distribution and not to a single point, the algorithm may produce some fluctuations even after it has converged. Because of this property, it may be more desirable to focus on *non*-convergence. Fortunately, there are non-convergence identifiers for other iterative algorithms, but the validity of these identifiers has not been systematically evaluated on imputation algorithms.

In this study, we explore different methods for identifying non-convergence in iterative imputation algorithms. We evaluate whether these methods are able to cover the extent of the non-convergence, and we also investigate the relation between non-convergence and the validity of the inferences. We translate the results of our simulation study into guidelines for practice, which we demonstrate by means of a case study.


# Simulation Set-Up

We investigate non-convergence in iterative imputation through model-based simulation in R (version 4.0.3; R Core Team 2020). We provide a summary of the simulation set-up in Algorithm 1; the complete script and technical details are available from [github.com/hanneoberman/MissingThePoint](https://github.com/hanneoberman/MissingThePoint).

**Algorithm 1: simulation set-up (pseudo-code)**

    for each simulation repetition 
      1. simulate complete data
      for each missingness condition 
        2. create missingness
        for each iteration 
          3. impute missingness
          4. perform analysis of scientific interest
          5. apply non-convergence identifiers
          6. pool results across imputations
          7. compute performance measures
        8. combine outcomes from all iterations
      9. combine outcomes from all missingness conditions
    10. aggregate outcomes from all simulation repetitions

## Aims

With this simulation, we assess the impact of non-convergence on the validity of scientific estimates obtained using the imputation package `{mice}` [@mice]. Inferential validity is reached when estimates are both unbiased and have nominal coverage across simulation repetitions ($n_{sim} = 1000$). To induce non-convergence, we terminate the algorithm after a varying number of iterations ($n_{it} = 1, 2, ..., 50$). We differentiate between nine different missingness scenarios that are defined by the data generating mechanism. 

## Data generating mechanism

Data are generated in each simulation repetition for a complete set of $n_{obs} = 1000$ cases (i.e., before inducing missingness). We define three multivariately normal random variables, let

$$
\begin{pmatrix}
Y\\
X_1\\
X_2
\end{pmatrix}
\sim \mathcal{N}
\begin{bmatrix}
\begin{pmatrix}
0\\
0\\
0
\end{pmatrix},
\begin{pmatrix}
2     &       &  \\
0.5   & 1     &  \\
-0.5  & 0.5   & 1
\end{pmatrix}
\end{bmatrix}.
$$ 

The complete set is amputed according to nine missingness conditions. We use a $3 \times 3$ factorial design consisting of three missingness mechanisms and three proportions of incomplete cases. 

- we use the three missingness mechanisms MCAR, MAR, and MNAR with defaults settings from `mice::ampute()` (e.g., right-tailed MAR)

- we use a multivariate missing data pattern and make 25%, 50%, and 75% of cases incomplete

## Estimands

We impute the missing data five times ($m = 5$) using Bayesian linear regression imputation with `mice` [@mice]. On each imputed dataset, we perform multiple linear regression to predict outcome variable $Y$ from the other two variables

$$
\hat{Y} = \beta_0 + \beta_1 X_1 + \beta_2 X_2,
$$

where $\hat{Y}$ is the predicted value of the outcome. Our estimands are the regression coefficient $\beta_1$ and coefficient of determination $\rho^2$ that we obtain after pooling the regression results across the imputations.

## Diagnostic methods

- non-convergence in iterative algorithms is diagnosed using an identifier and a parameter

- the parameter can be any statistic that we track across iterations, for example the average imputed value per imputation (i.e., chain means)

- the identifier is a calculation of some sort that quantifies non-convergence

- identifiers are historically focused on either non-mixing between chains or non-stationarity within chains

- the popular non-mixing identifier rhat has recently been updated by Vehtari et al, and should now work for non-stationarity as well

- just to be sure, we also use autocorrelation to quantify trending within chains

- we apply these identifiers to four different univariate and multivariate parameters 

- the univariate parameters are those commonly used for visual inspection: chain means and chain variances

- one multivariate parameter that is of immediate interest for our estimand is the estimated value itself: regression coefficient beta_1

- we also propose a new multivariate parameter that is not dependent on the model of scientific interest: the first eigenvalue of the variance-covariance matrix of the completed data [explain!]

We use eight different methods to diagnose non-convergence: a combination of two non-convergence identifiers---autocorrelation and $\widehat{R}$---and four parameters---chain means, chain variances, $\beta_1$, and $\lambda$.

## Performance measures

As recommended by @buur18, our performance measures are bias, confidence interval width, and coverage rate of the estimands ($\S$ 2.5.2).
<!-- [or just of the regression estimate?? otherwise add the ciw and cov of r2 too!!]. -->

# Simulation Results

The following figures display the simulation results for the eight diagnostic methods and four performance measures, contrasted to the number of iterations in the imputation algorithm. Within the figures, we split the results according to the missingness conditions [missingness mechanisms as line types, and proportion of incomplete cases as colors]. Note that these results are averages of the $n_{sim} = 1000$ simulation repetitions.

## Diagnostic Methods

Figure \@ref(fig:ac) shows the autocorrelations in the chain means (panel A), chain variances (panel B), regression estimates (panel C), and eigenvalues (panel D).

```{r ac, fig.cap = "Autocorrelation (AC) with different parameters.", echo=FALSE}
ac_means <- simulation_plot(v="ac.max.mu.Y", name = "AC chain means") + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_vars <- simulation_plot(v="ac.max.sigma.Y", name = "AC chain variances") + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_betas <- simulation_plot(v="ac.max.qhat", name = quote("AC"~beta)) + ggplot2::scale_y_continuous(limits = c(0.175, 1))
ac_lambdas <- simulation_plot(v="ac.max.lambda", name = quote("AC"~lambda)) + ggplot2::scale_y_continuous(limits = c(0.175, 1))
(ac_means + ac_vars) / (ac_betas + ac_lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```

Autocorrelation in the chain means decreases rapidly in the first few iterations (see \@ref(fig:ac)A). The decrease is substantive until $n_{it} \geq 6$. This means that there is some initial trending within chains, but the average imputed value quickly reaches stationarity. These results hold irrespective of the missingness condition.


Autocorrelation in the chain variances show us something similar (see \@ref(fig:ac)B). The number of iterations that is required to reach non-improving autocorrelations is somewhat more ambiguous than for chain means, but generally around $n_{it} \geq 10$. We do not observe a systematic difference between missingness conditions here either. 


There is more autocorrelation in the scientific estimates than in the chain means and chain variances (see \@ref(fig:ac)C). We observe the highest autocorrelations in conditions where 75% of cases are incomplete. Overall, the autocorrelations reach a plateau when $n_{it} \geq 20$ to $30$. There is no clear effect of the missingness mechanisms. 


The autocorrelation in the eigenvalues exhibits a similar trend to the autocorrelation in the scientific estimates (see \@ref(fig:ac)D). Trending in this parameter diminishes when $n_{it} \geq 20$.

Figure \@ref(fig:rh) shows the potential scale reduction factor in the chain means (panel A), chain variances (panel B), regression estimates (panel C), and eigenvalues (panel D).

```{r rh, fig.cap = "Potential scale reduction factor (Rh) with different parameters.", echo=FALSE}
rh_means <- simulation_plot(v="r.hat.max.mu.Y", name = "Rh chain means") + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_vars <- simulation_plot(v="r.hat.max.sigma.Y", name = "Rh chain variances") + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_betas <- simulation_plot(v="r.hat.max.qhat", name = quote("Rh"~beta)) + ggplot2::scale_y_continuous(limits = c(1, 1.65))
rh_lambdas <- simulation_plot(v="r.hat.max.lambda", name = quote("Rh"~lambda)) + ggplot2::scale_y_continuous(limits = c(1, 1.65))
(rh_means + rh_vars) / (rh_betas + rh_lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```


We observe that $\widehat{R}$-values of the chain means generally decreases as a function of the number of iterations (see \@ref(fig:rh)A). An exception to this observation is the initial increase when $3 \leq n_{it} \leq 5$ [interpret?? due to initialization or is there really more mixing initially??]. After the first couple of iterations, the mixing between chain means generally improves until $n_{it} \geq 30$ to $40$. There is no apparent differentiation between the missingness conditions. 


The mixing between chain variances mimics the mixing between chain means almost perfectly (see \@ref(fig:rh)B). Irrespective of the missingness condition, the $\widehat{R}$-values taper off around $n_{it} \geq 30$. 


With the regression estimate as parameter we observe very similar $\widehat{R}$-values than with chain means and chain variances (see \@ref(fig:rh)C). We do, however, see some differences between missingness conditions. Conditions where 75% of the cases are incomplete show more extreme non-mixing. The overall trend remains the same: about 30 iterations are required before mixing stops improving substantially.


$\widehat{R}$-values of the eigenvalues show a trend similar to the chain means and chain variances (see \@ref(fig:rh)D). [add something about conditions??]

[These rhat plots all show some initialization before the fifth iteration: is rhat useful before that??]

## Performance Measures

In Figure \@ref(fig:perf) we show the performance measures: bias in the regression estimate (panel A), bias in the coefficient of determination (panel B), the empirical coverage rate of the regression estimate (panel C) and the average confidence interval width of this estimate (panel D).

```{r perf, fig.cap = "Performance measures.", echo=FALSE}
est <- simulation_plot(v = "est", name = "Bias in regression estimate") + 
  list(geom_hline(yintercept = 0, color = "grey")) 
cov <- simulation_plot( v ="cov", name = "Coverage rate") + 
  list(geom_hline(yintercept = 0.95, color = "grey"))
ciw <- simulation_plot(v="CIW", name = "Average CI width") 
rsq <- simulation_plot(v="rsq", name = "Bias in coefficient of determination") +
  list(geom_hline(yintercept = 0, color = "grey"))
(est + rsq) / (cov + ciw) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')
```

We see that within a few iterations the bias in the regression estimate approaches zero (see \@ref(fig:perf)A). When $n_{it} \geq 6$, even the worst-performing conditions (e.g., with a proportion of incomplete cases of 75%) produce stable, non-improving estimates [regression coefficient is underestimated because there is less info to estimate the relation??]. 

Equivalent to the bias in the regression estimate, the bias in the coefficient of determination tapers off within a couple of iterations (see \@ref(fig:perf)B). We observe stable estimates in all conditions when $n_{it} \geq 6$ [interpret the over-estimation in MNAR+75% condition?].

Nominal coverage is quickly reached (see \@ref(fig:perf)C). After just three iterations, the coverage rates are non-improving in every missingness condition [but MNAR with 5% incomplete cases does not reach nominal coverage --\> due to bias in the estimate in combination with very narrow CI (see CIW!)].


The average confidence interval width decreases quickly with every added iteration until a stable plateau is reached (see \@ref(fig:perf)D). Depending on the proportion of incomplete cases this takes up-to $n_{it} \geq 9$. 


# Discussion

Our study found that---in the cases considered---inferential validity was achieved after five to ten iterations, much earlier than indicated by the non-convergence identifiers. Of course, it never hurts to iterate longer, but such calculations hardly bring added value.


- Convergence diagnostics keep improving substantially until n_it = 20-30

- Performance measures do not improve after n_it = 9

- [methodological explanation is that rhat and ac have a lag (few it to inform your statistic) --\> will always indicate convergence slower than inferential validity is reached]

- Univariate thetas may under-estimate non-convergence. 

- Determining non-stationarity with lambda is more difficult than with qhat :(


In short, the validity of iterative imputation stands or falls with algorithmic convergence---or so it's thought. We have shown that iterative imputation algorithms can yield correct outcomes even when a converged state has not yet formally been reached. Any further iterations just burn computational resources without improving the statistical inferences. 

# Case Study

- We use real data: the `boys` dataset from the `mice` package

- We are interested in predicting age from the other variables, in particular in the regression coefficient of `hgt`

- We compare non-convergence identified using visual inspection versus rhat in the chain variances, scientific estimate and lambda.

- The figures show results of a `mice` run with 20 iterations but otherwise default settings.

```{r case, fig.cap = "Case study.", echo=FALSE}
trace <- param %>% 
  ggplot() + 
  geom_line(aes(x = it, y = mu.hgt, color = as.factor(m))) + 
  theme_classic() + 
  labs(x = "Iteration number",
       y = "Chain mean variable 'hgt'",
       color = "Imputation")
means <- casestudy_plot(v="r.hat.max.mu.hgt", name = "Rhat chain means") + 
  list(geom_hline(yintercept = 1.2, color = "grey"))
qhats <-
  casestudy_plot(v = "r.hat.max.qhat", name = "Rhat regression estimate") +
  list(geom_hline(yintercept = 1.2, color = "grey"))
lambdas <- casestudy_plot(v = "r.hat.max.lambda", name = "Rhat lambda") +
  list(geom_hline(yintercept = 1.2, color = "grey"))
(trace + means) / (qhats + lambdas) + plot_layout(guides="collect") + plot_annotation(tag_levels = 'A')

```

From the traceplot of the chain means (see \@ref(fig:case)A) it seems that mixing improves up-to 10 iterations, while trending is only apparent in the first three iterations. 



This figure (\@ref(fig:case)B) shows that 7 iterations are required before the $\widehat{R}$-values of the chain means drop below the threshold for non-convergence.


The $\widehat{R}$-values for the scientific estimate reaches the threshold much sooner, when $n_{it}=14$ (see \@ref(fig:case)C).


According to the $\widehat{R}$-values with $\lambda$ as parameter, at least 15 iterations are required (see \@ref(fig:case)D).


# References 
